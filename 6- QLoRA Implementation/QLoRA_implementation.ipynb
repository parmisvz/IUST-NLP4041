{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# QLoRA Fine-tuning GPT-Neo 125M on Tiny Shakespeare (Complete Notebook)\n",
        "\n",
        "This notebook:\n",
        "\n",
        "- Loads **EleutherAI/gpt-neo-125M**\n",
        "- Downloads **Tiny Shakespeare**\n",
        "- Trains with **QLoRA**: 4-bit quantized base model + LoRA adapters (PEFT)\n",
        "- Saves adapters\n",
        "- Shows inference after training\n",
        "\n",
        "Optional section at the end: **LoRA without quantization**.\n",
        "\n",
        "## Requirements\n",
        "\n",
        "- CUDA GPU recommended for 4-bit training.\n",
        "- If you're on CPU, QLoRA may not work (bitsandbytes 4bit generally expects GPU).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import os\n",
        "# import math\n",
        "# import requests\n",
        "# import torch\n",
        "\n",
        "# from datasets import Dataset\n",
        "# from torch.utils.data import DataLoader\n",
        "\n",
        "# from transformers import (\n",
        "#     AutoTokenizer,\n",
        "#     AutoModelForCausalLM,\n",
        "#     BitsAndBytesConfig,\n",
        "#     DataCollatorForLanguageModeling,\n",
        "#     get_linear_schedule_with_warmup,\n",
        "# )\n",
        "\n",
        "# # from huggingface_hub import login\n",
        "# # from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "# print(\"Torch:\", torch.__version__)\n",
        "# print(\"CUDA available:\", torch.cuda.is_available())\n",
        "# if torch.cuda.is_available():\n",
        "#     print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "#     print(\"Compute capability:\", torch.cuda.get_device_capability(0))\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# hf_token = os.environ.get('HF_TOKEN', '').strip()\n",
        "# if hf_token:\n",
        "#     login(token=hf_token)\n",
        "#     print('Logged in using HF_TOKEN env var')\n",
        "# else:\n",
        "#     print('No HF_TOKEN env var found. If model download requires auth, login interactively:')\n",
        "#     # Uncomment to login interactively:\n",
        "#     # login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Config\n",
        "\n",
        "# MODEL_NAME = \"EleutherAI/gpt-neo-125M\"\n",
        "\n",
        "# # Data\n",
        "# MAX_LENGTH = 512\n",
        "# CHUNK_SIZE_CHARS = 2000  # chunk raw text into segments\n",
        "\n",
        "# # Training\n",
        "# BATCH_SIZE = 2\n",
        "# GRAD_ACCUM_STEPS = 8\n",
        "# EPOCHS = 1\n",
        "# LR = 2e-4\n",
        "# WARMUP_RATIO = 0.03\n",
        "# WEIGHT_DECAY = 0.0\n",
        "# MAX_STEPS_LIMIT = None  # set to an int to cap steps for quick tests\n",
        "\n",
        "# # LoRA\n",
        "# LORA_R = 16\n",
        "# LORA_ALPHA = 32\n",
        "# LORA_DROPOUT = 0.1\n",
        "\n",
        "# OUTPUT_DIR = \"qlora_gpt_neo_adapters\"\n",
        "\n",
        "# print(\"MODEL_NAME:\", MODEL_NAME)\n",
        "# print(\"MAX_LENGTH:\", MAX_LENGTH)\n",
        "# print(\"BATCH_SIZE:\", BATCH_SIZE)\n",
        "# print(\"GRAD_ACCUM_STEPS:\", GRAD_ACCUM_STEPS)\n",
        "# print(\"LR:\", LR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Load tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "print(\"pad_token:\", tokenizer.pad_token, \" | eos_token:\", tokenizer.eos_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Load model in 4-bit (QLoRA base)\n",
        "\n",
        "This loads the base model quantized to 4-bit NF4 using bitsandbytes.\n",
        "\n",
        "> If you're on CPU-only, this may fail. QLoRA is intended for GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compute_dtype = (\n",
        "#     torch.bfloat16\n",
        "#     if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "#     else torch.float16\n",
        "# )\n",
        "# print(\"compute_dtype:\", compute_dtype)\n",
        "\n",
        "# bnb_config = BitsAndBytesConfig(\n",
        "#     load_in_4bit=True,\n",
        "#     bnb_4bit_quant_type=\"nf4\",\n",
        "#     bnb_4bit_use_double_quant=True,\n",
        "#     bnb_4bit_compute_dtype=compute_dtype,\n",
        "# )\n",
        "\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     MODEL_NAME,\n",
        "#     quantization_config=bnb_config,\n",
        "#     device_map=\"auto\",\n",
        "# )\n",
        "\n",
        "# # Prepare for k-bit training\n",
        "# model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# print(\"Loaded quantized model.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Choose LoRA target modules\n",
        "\n",
        "GPT-Neo module names can vary across implementations. We'll:\n",
        "\n",
        "1. Print some likely linear module names\n",
        "2. Try a robust target list\n",
        "3. If PEFT warns about missing modules, adjust `target_modules` accordingly.\n",
        "\n",
        "If you want, you can run the listing cell and then set `TARGET_MODULES` manually.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect a handful of module names to help target LoRA correctly\n",
        "interesting = []\n",
        "for name, module in model.named_modules():\n",
        "    # Show likely projection layers\n",
        "    if any(k in name for k in [\"attn\", \"mlp\", \"proj\", \"c_fc\", \"c_proj\"]) and hasattr(\n",
        "        module, \"weight\"\n",
        "    ):\n",
        "        t = type(module).__name__\n",
        "        interesting.append((name, t))\n",
        "\n",
        "print(\"Sample candidate modules (first 60):\")\n",
        "for n, t in interesting[:60]:\n",
        "    print(f\"  {n}  | {t}\")\n",
        "\n",
        "# A common robust set for GPT-style blocks\n",
        "TARGET_MODULES = [\n",
        "    \"attn.attention\",\n",
        "    \"attn.proj\",\n",
        "    \"mlp.c_fc\",\n",
        "    \"mlp.c_proj\",\n",
        "]\n",
        "print(\"TARGET_MODULES:\", TARGET_MODULES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Attach LoRA adapters (QLoRA)\n",
        "\n",
        "Only LoRA weights will be trainable; the quantized base stays frozen.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=TARGET_MODULES,\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Download Tiny Shakespeare + build dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "raw_text = requests.get(url, timeout=60).text\n",
        "print(\"Chars:\", len(raw_text))\n",
        "\n",
        "# Chunking raw text into bigger segments helps the model learn context\n",
        "chunks = [\n",
        "    raw_text[i : i + CHUNK_SIZE_CHARS]\n",
        "    for i in range(0, len(raw_text), CHUNK_SIZE_CHARS)\n",
        "]\n",
        "dataset = Dataset.from_dict({\"text\": chunks})\n",
        "print(\"Dataset chunks:\", len(dataset))\n",
        "\n",
        "\n",
        "def tokenize_fn(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "    )\n",
        "\n",
        "\n",
        "tokenized = dataset.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "train_loader = DataLoader(\n",
        "    tokenized, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collator\n",
        ")\n",
        "\n",
        "batch0 = next(iter(train_loader))\n",
        "print({k: v.shape for k, v in batch0.items()})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Training loop (gradient accumulation + scheduler)\n",
        "\n",
        "- Uses AdamW\n",
        "- Gradient accumulation\n",
        "- Linear warmup schedule\n",
        "\n",
        "Note: With PEFT + QLoRA, only adapter params update.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "model.train()\n",
        "\n",
        "# Optimizer over trainable (LoRA) params\n",
        "optimizer = AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "steps_per_epoch = math.ceil(len(train_loader) / GRAD_ACCUM_STEPS)\n",
        "total_steps = steps_per_epoch * EPOCHS\n",
        "if MAX_STEPS_LIMIT is not None:\n",
        "    total_steps = min(total_steps, MAX_STEPS_LIMIT)\n",
        "\n",
        "warmup_steps = int(total_steps * WARMUP_RATIO)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps,\n",
        ")\n",
        "\n",
        "print(\"steps_per_epoch:\", steps_per_epoch)\n",
        "print(\"total_steps:\", total_steps)\n",
        "print(\"warmup_steps:\", warmup_steps)\n",
        "\n",
        "global_step = 0\n",
        "running_loss = 0.0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    for step, batch in enumerate(train_loader):\n",
        "        # respect step limit\n",
        "        if MAX_STEPS_LIMIT is not None and global_step >= MAX_STEPS_LIMIT:\n",
        "            break\n",
        "\n",
        "        # device_map='auto' -> model already on correct device(s)\n",
        "        batch = {k: v.to(model.device) for k, v in batch.items()}\n",
        "\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss / GRAD_ACCUM_STEPS\n",
        "        loss.backward()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if (step + 1) % GRAD_ACCUM_STEPS == 0:\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            global_step += 1\n",
        "            if global_step % 10 == 0:\n",
        "                avg_loss = running_loss / 10\n",
        "                running_loss = 0.0\n",
        "                print(\n",
        "                    f\"Epoch {epoch+1} | Step {global_step}/{total_steps} | loss {avg_loss:.4f}\"\n",
        "                )\n",
        "\n",
        "            if global_step >= total_steps:\n",
        "                break\n",
        "\n",
        "    if global_step >= total_steps:\n",
        "        break\n",
        "\n",
        "print(\"Training finished.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Save adapters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "print(\"Saved to:\", OUTPUT_DIR)\n",
        "print(\"Files:\", os.listdir(OUTPUT_DIR))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) Inference test (using current in-memory model)\n",
        "\n",
        "This uses the trained adapters currently attached to the quantized base.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "prompt = \"ROMEO:\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    out = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=120,\n",
        "        do_sample=True,\n",
        "        temperature=0.9,\n",
        "        top_p=0.95,\n",
        "    )\n",
        "\n",
        "print(tokenizer.decode(out[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12) Reload from disk (fresh session test)\n",
        "\n",
        "This cell shows how to reload the base quantized model and the saved adapters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "bnb_config_reload = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        ")\n",
        "\n",
        "base_reload = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config_reload,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "tok_reload = AutoTokenizer.from_pretrained(OUTPUT_DIR, use_fast=True)\n",
        "tok_reload.pad_token = tok_reload.eos_token\n",
        "\n",
        "model_reload = PeftModel.from_pretrained(base_reload, OUTPUT_DIR)\n",
        "model_reload.eval()\n",
        "\n",
        "prompt = \"JULIET:\"\n",
        "inputs = tok_reload(prompt, return_tensors=\"pt\").to(model_reload.device)\n",
        "with torch.no_grad():\n",
        "    out = model_reload.generate(\n",
        "        **inputs, max_new_tokens=120, do_sample=True, temperature=0.9, top_p=0.95\n",
        "    )\n",
        "print(tok_reload.decode(out[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Optional: LoRA without quantization (fp16/bf16)\n",
        "\n",
        "If you also want a “classic LoRA” run (not QLoRA), you can use the following section.\n",
        "\n",
        "This loads the base model in fp16/bf16 and attaches LoRA adapters similarly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LoRA (no 4-bit quantization)\n",
        "\n",
        "from peft import PeftModel\n",
        "\n",
        "USE_CLASSIC_LORA = False  # set True to run\n",
        "\n",
        "if USE_CLASSIC_LORA:\n",
        "    base_fp = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=compute_dtype,\n",
        "    ).to(device)\n",
        "\n",
        "    # Attach LoRA\n",
        "    lora_config_fp = LoraConfig(\n",
        "        r=LORA_R,\n",
        "        lora_alpha=LORA_ALPHA,\n",
        "        lora_dropout=LORA_DROPOUT,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=TARGET_MODULES,\n",
        "    )\n",
        "    lora_fp_model = get_peft_model(base_fp, lora_config_fp)\n",
        "    lora_fp_model.print_trainable_parameters()\n",
        "\n",
        "    # quick train loop\n",
        "    lora_fp_model.train()\n",
        "    opt = AdamW(lora_fp_model.parameters(), lr=LR)\n",
        "    for epoch in range(1):\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            out = lora_fp_model(**batch)\n",
        "            loss = out.loss\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            if i % 50 == 0:\n",
        "                print(\"step\", i, \"loss\", float(loss))\n",
        "\n",
        "    lora_fp_model.save_pretrained(\"lora_fp_gpt_neo_adapters\")\n",
        "    tokenizer.save_pretrained(\"lora_fp_gpt_neo_adapters\")\n",
        "    print(\"Saved classic LoRA adapters to lora_fp_gpt_neo_adapters\")\n",
        "else:\n",
        "    print(\"Classic LoRA section skipped (set USE_CLASSIC_LORA=True to run).\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
